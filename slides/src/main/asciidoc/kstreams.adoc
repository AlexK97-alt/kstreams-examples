= Kafka Streams API: Шаг за рамки Hello World
Иван Пономарёв, КУРС/МФТИ
:revealjs_theme: black
:revealjs_customtheme: white_course.css
:revealjs_slideNumber:
:revealjs_history:
:revealjs_progress:
:encoding: UTF-8
:lang: ru
include::_doc_general_attributes.adoc[]
:doctype: article
:toclevels: 3
:imagesdir: images
:source-highlighter: highlightjs
:highlightjsdir: highlight
:icons: font
:iconfont-remote!:
:iconfont-name: font-awesome-4.7.0/css/font-awesome
:revealjs_mouseWheel: true
:revealjs_center: false

//== Часть 1. Введение
:!figure-caption:

== Зачем нам Kafka?

[%step]
* Web-scraping в реальном времени
* 500 запросов/сек
* Удобные абстракции «из коробки»:
** «Подрезаемый» лог
** Microbatching
** Автоматическая балансировка при масштабировании
* DISCLAIMER: пока не в production

== Kafka за 30 секунд
.Источник: Kafka. The Definitive Guide
image::kafka_cluster.png[{image-60-width}]

== Kafka Message

image::1024px-Aiga_mail.svg.png[{image-10-width}]

* Partition Number -- определяется Producer-ом
* Ключ
* Значение

== Compacted topics
.Источник: Kafka Documentation
image::log_compaction.png[{image-70-width}]

== Kafka Streams API: нам обещают...

[%step]
* Real-time stream processing
* Stream-like API (map / reduce)
* Под капотом:
** Автоматический коммит оффсетов
** Ребалансировка
** Внутреннее состояние обработчиков
** Легкое масштабирование


== Попробуем?

image::gamov.png[{image-40-width}]

== Kafka Streams API: общая структура KStreams-приложения


[source,java]
----
StreamsConfig config = ...;
//Здесь устанавливаем всякие опции

StreamsBuilder builder = ...; 
//Здесь строим топологию


//Это за нас делает SPRING-KAFKA
KafkaStreams streams = new KafkaStreams(builder, config); 
streams.start(); 
...
streams.close();

----

== Топология (а на самом деле, DAG)

[graphviz,"topology-sample.png"]
----
digraph G {


node [shape="circle" ];


a[label="source"];
b[label="source"];
c[label="branch /\nsplit"];

d[label="process"];
e[label="join /\nmerge"];
f[label="sink"];
g[label="sink"];
{rank = same; a; b;}
a->c;
b->e;
c->e;
c->d;
d->f;
e->g;

}
----


== С чего начать?

image::initializr.png[{image-90-width}]


== @Bean KafkaConfiguration 

[source,java]
----
//ВАЖНО!
@Bean(name = 
    KafkaStreamsDefaultConfiguration
                .DEFAULT_STREAMS_CONFIG_BEAN_NAME)
public KafkaStreamsConfiguration getStreamsConfig() {
    Map<String, Object> props = new HashMap<>();
    //ВАЖНО!
    props.put(StreamsConfig.APPLICATION_ID_CONFIG, "my-app");
    //ВАЖНО!
    props.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 4);
    props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
    ...
    KafkaStreamsConfiguration streamsConfig = 
            new KafkaStreamsConfiguration(props);
    return streamsConfig;
}
----

== @Bean NewTopic

[source,java]
----
@Bean
NewTopic getFilteredTopic() {
    Map<String, String> props = new HashMap<>();
    props.put(
      TopicConfig.CLEANUP_POLICY_CONFIG,
      TopicConfig.CLEANUP_POLICY_COMPACT);
    return new NewTopic("mytopic", 10, (short) 1).configs(props);
}
----


== @Bean Topology
[graphviz, "yelling-topology.png"]
-----
digraph G {

rankdir="LR";
node [fontsize=16; shape="circle"; fixedsize="true"; width="1.1"];

Source -> MapValues -> Sink

}
-----
{nbsp} +
[source,java]
----
@Bean
public Topology createTopology(StreamsBuilder streamsBuilder) {

    KStream<String, String> foo = streamsBuilder
        .stream("foo",Consumed.with(Serdes.String(), Serdes.String()))
        .mapValues((ValueMapper<String, String>) String::toUpperCase)
        .to("bar", Produced.with(Serdes.String(), Serdes.String()));
        return streamsBuilder.build();
}
----


== TopologyTestDriver: создание

[source,java]
----
KafkaStreamsConfiguration config = new KafkaConfiguration()
                                        .getStreamsConfig();
StreamsBuilder sb = new StreamsBuilder();
Topology topology = new TopologyConfiguration().createTopology(sb);
TopologyTestDriver topologyTestDriver = 
        new TopologyTestDriver(topology, 
                               config.asProperties());
----

== TopologyTestDriver: использование

[source,java]
----
ConsumerRecordFactory<String, String> factory = new
                ConsumerRecordFactory<>(...);
topologyTestDriver.pipeInput(factory.create("foo", "hello", "world"));

ProducerRecord<String, String> bar = 
    topologyTestDriver.readOutput("bar", ...);

assertEquals("hello", bar.key());
assertEquals("WORLD", bar.value());
----

== Демо

* Real-time processing
* Сохранение consumer offset

== Простое ветвление стримов
Java-стримы так не могут:
[source,java]
----
KStream<..> foo = ...
KStream<..> bar = foo.mapValues(…).map... to...
Kstream<..> baz = foo.filter(…).map... forEach...
----
[graphviz, "simplebranch.png"]
-----
digraph G {
rankdir="LR";
node [fontsize=16; shape="circle"; fixedsize="true"; width="0.5"; label=""];
Start, BF, CF, DF [style="invis"]
Start -> A
A -> B -> BF
A -> C -> CF
}
-----

== Ветвление стримов по условию

Не используйте `KStream.branch`, используйте `KafkaStreamsBrancher`!
[source,java]
----
new KafkaStreamsBrancher<String, String>()
   .branch((key, value) -> value.contains("A"), ks -> ks.to("A"))
   .branch((key, value) -> value.contains("B"), ks -> ks.to("B"))
   .defaultBranch(ks -> ks.to("C"))
   .onTopOf(builder.stream("source"))
   .map(...)
----
[graphviz, "switchbranch.png"]
-----
digraph G {
rankdir="LR";
node [fontsize=16; shape="circle"; fixedsize="true"; width="0.5"; label=""];
Start, BF, CF, DF [style="invis"]
Branch [shape="diamond", label="?"]
Start -> A -> Branch
Branch -> B -> BF
Branch -> C -> CF
}
-----

== Простое слияние
[source,java]
----
KStream<String, Integer> foo = ... 
KStream<String, Integer> bar = ...
KStream<String, Integer> merge = foo.merge(bar);
----
[graphviz, "merge.png"]
-----
digraph G {
rankdir="LR";
node [fontsize=16; shape="circle"; fixedsize="true"; width="0.5"; label=""];
Finish, BF, CF, DF [style="invis"]
BF -> A
CF -> B 
A -> C -> Finish
B -> C 
}
-----

== Локальное состояние

Facebook's RocksDB -- что это и зачем?

[.custom-style]
[cols="30a,70a"]
|===
.<|image::rocksdb.png[]
.<|
* Embedded key/value storage
* LSM Tree (Log-Structured Merge-Tree)
* High-performant (data locality)
* Persistent, optimized for SSD
|===


== RocksDB: краткий гид

* Сохранение K,V в бинарном формате
* Лексикографическая сортировка
* Iterator (snapshot view)
* Удаление диапазона (deleteRange)

== Пишем “Tweet Counting App”

* Подсчитаем твиты с хэштегом
* Key: имя пользователя, value: Count
* `KeyValueStore` и `Transformer`

== @Bean Topology
[graphviz, "counting-topology.png"]
-----
digraph G {
rankdir="LR";
node [fontsize=18; shape="circle"; fixedsize="true"; width="1.1"];
Store [shape="cylinder"; label="Local Store"; fixedsize="true"; width="1.5"]
Source -> Filter -> Counter -> Sink
Counter -> Store [dir=both; label=" \n "]
{rank = same; Store; Counter;}
}
-----
{nbsp} +
[source,java]
----
KStream<String, String> filtered = streamsBuilder
            .stream("in")
            .filter((k, v) -> v.toLowerCase().contains("jpoint"));

KStream<String, Integer> counted =
    new CountingTransformer()
        .transformStream(streamsBuilder, filtered);
----

== Подсчёт твитов
[source,java]
----
@Override
public KeyValue<String, Integer> transform(
    String key, 
    String value, 
    KeyValueStore<String, Integer> stateStore) {
    int current = Optional.ofNullable(
            stateStore.get(key)).orElse(0) + 1;
    stateStore.put(key, current);
    return KeyValue.pair(key, current);
}
----
== Демо: Ребалансировка / репликация

* Ребалансировка / репликация партиций state при запуске / выключении обработчиков.

== Сохранение локального состояния в{nbsp}топик

[source,code]
----
$kafka-topics --describe counting-demo-app-jpoint-counted-changelog
Topic:counting-demo-app-jpoint-counted-changelog
PartitionCount:10       
ReplicationFactor:1 
Configs:cleanup.policy=compact
----

== Партиционирование и local state

[graphviz, "local-partitioning.png"]
-----
digraph D {
  subgraph system {

    subgraph cluster_p1 {
      label = "Worker 1";
        
          
          subgraph cluster_pp11{
              label = "Partition 2"
          
              b [label = "B"];
              c[label = "C"];
              
          }
 
        subgraph cluster_pp1{
              label = "Partition 1"
              
              
              a [label = "A"];
              
          }
          
        
        ls1[shape="cylinder" label = "RocksDB"]
        a->ls1;
        b->ls1;
        c->ls1;
        
        p1[shape="plaintext" label = "Partition 1"];
        p2[shape="plaintext" label = "Partition 2"];
        ls1->p1[dir="both"];
        ls1->p2[dir="both"];
    }
    subgraph cluster_p2 {
      label = "Worker 2";
      subgraph cluster_pp2{
              label = "Partition 3"
                
              d[label = "D"];
              
              
          }
          
          
        ls2[shape="cylinder" label = "RocksDB"]
        d->ls2;
        p3[shape="plaintext" label = "Partition 3"];
        ls2->p3[dir="both"];
    }
    
    
  }
} 
-----

== Репартиционирование
[graphviz, "through.png"]
-----
digraph G
{
    rankdir="LR";
    node [shape=record, width=.1, height=.1];
    node1 [label="{ | | | | }", fontsize = 18, xlabel= "through(. . .)"];
    
    node [label = " "; shape="circle"; fixedsize="true"; width="1.1"];
    Source -> node1
    node1 -> Sink
    
}
-----
{nbsp} +

* Явное при помощи +
`through(String topic, Produced<K, V> produced)`
* Неявное при `map()`/`selectKey()` и stateful-операциях

== Таблицы vs стримы

Местонахождение пользователя

.Michael G. Noll. Of Streams and Tables in Kafka and Stream Processing
image::stream-table-animation-latestLocation.gif[{image-100-width}]

== Таблицы vs стримы

Количество посещенных мест

.Michael G. Noll. Of Streams and Tables in Kafka and Stream Processing
image::stream-table-animation-numVisitedLocations.gif[{image-100-width}]


== Таблицы vs стримы

Производная и интеграл

.Martin Kleppmann, “Designing Data Intensive Applications”
image::derivative-and-integral.png[{image-100-width}]


== Переписываем counting app при помощи KTable

[source,java]
----
KTable<String, Long> count = filtered.groupByKey().count();

count.toStream().foreach((k, v) -> {
            gui.update(k, String.valueOf(v));
        });
----

[source,code]
----
$kafka-topics --describe
Topic:table-demo-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog 
PartitionCount:10
ReplicationFactor:1
Configs:cleanup.policy=compact
----


== Извлекаем самый свежий твит пользователя про #jpoint

[source,java]
----
KTable<String, String> latest = filtered.groupByKey()
            .reduce((v1, v2) -> v2);
latest.toStream()....
----
[source,code]
----
$kafka-topics --describe
Topic:table-demo-KSTREAM-REDUCE-STATE-STORE-0000000002-changelog 
PartitionCount:10
ReplicationFactor:1
Configs:cleanup.policy=compact
----



== Join: объединяем источники

(например, твиты и геолокация)

image::derivative.png[{image-60-width}]

== Join: объединяем источники
[graphviz, "join-storages.png"]
-----
digraph G {
rankdir="LR";
node [fontsize=18; shape="circle"; fixedsize="true"; width="1.1"];
Store1 [shape="cylinder"; label="Local Store 1"; fixedsize="true"; width="1.7"]
Store2 [shape="cylinder"; label="Local Store 2"; fixedsize="true"; width="1.7"]
Source1 -> Join
Source2 -> Join

Join -> Sink
Join -> Store1 [dir=both; label=" \n "]
Join -> Store2 [dir=both; label=" \n "]
Store1 -> Store2 [style=invis]
{rank = same; Store1; Join }
}
-----

== Демо: Объединяем твиты с геолокацией
[source,java]
----
KTable<String, String> geo = streamsBuilder
        .table("geo", ...);
KTable<String, String> joined = latest
    .join(geo, (v1, v2) -> String.format("%s: %s", v2, v1));
----

[source,code]
----
$kafka-topics --list
table-demo-KSTREAM-REDUCE-STATE-STORE-0000000002-changelog
table-demo-geo-STATE-STORE-0000000006-changelog
----

== Копартиционирование

Join работает


[graphviz, "copart-norm.png"]
-----
digraph D {
  subgraph system {
     subgraph cluster_s2{
          style = "invis"
          S1 [shape=plaintext label = "Source 1"];
          S2 [shape=plaintext label = "Source 2"];
          S1->S2 [style="invis"]
      }
    subgraph cluster_p1 {
      label = "Worker 1";
        
          
          subgraph cluster_pp11{
              label = "Partition 2"
          
              b [label = "B"];
              c[label = "C"];
              
          }
          subgraph cluster_pp12{
              label = "Partition 2"
              labelloc ="b"
          
              b1 [label = "B"];
              
              c1[label = "C"];
          }
          
          subgraph cluster_p1{
              label = "Partition 1"
          labelloc = "b"
              
              a1 [label = "A"]
              
          }
          
        subgraph cluster_pp1{
              label = "Partition 1"
              
              
              a [label = "A"];
              
          }
          
          a->a1[style="dashed" dir="none"];
          b->b1[style="dashed" dir="none"];
          c->c1[style="dashed" dir="none"];
    }
    subgraph cluster_p2 {
      label = "Worker 2";
      subgraph cluster_pp2{
              label = "Partition 3"
                
              d[label = "D"];
              
              
          }
          subgraph cluster_p2{
              label = "Partition 3"
              labelloc = "b"
              d1[label = "D"];
              
              
          }
          
          d->d1[style="dashed" dir="none"];
    }
  }
} 
-----

== Несовпадение количества партиций

Join не работает (Runtime Exception)

[graphviz, "copart-diff.png"]
-----
digraph D {
  subgraph system {
     subgraph cluster_s2{
          style = "invis"
          S1 [shape=plaintext label = "Source 1"];
          S2 [shape=plaintext label = "Source 2"];
          S1->S2 [style="invis"]
      }
    subgraph cluster_p1 {
      label = "Worker 1";
        subgraph cluster_p1{
              label = "Partition 1"
              labelloc = "b"
              b1 [label = "B"]
              a1 [label = "A"]
          }
          
        subgraph cluster_pp1{
              label = "Partition 1"
          
              
              a [label = "A"];
              
          }
          
          subgraph cluster_pa2{
              label = "Partition 2"
          b [label = "B"];
              c [label = "C" color="red"];
              
          }
          a->a1[style="dashed" dir="none"];
          b->b1[style="dashed" dir="none"];
          
          
    }
    subgraph cluster_p2 {
      label = "Worker 2";
      subgraph cluster_pp2{
              label = "Partition 3"
          
              d[label = "D"];
              
              
          }
          subgraph cluster_pa3{
              label = "Partition 2"
              labelloc = "b"
          
              d1[label = "D"];
              c1[label = "C" color ="red"];
              
          }
          c->c1[ dir="none" color="red"];
          d->d1[style="dashed" dir="none"];
    }
  }
} 
-----

== Несовпадение алгоритма партицирования

Join не работает молча!

[graphviz, "copart-diff-algorithm.png"]
-----
digraph D {
  subgraph system {
     subgraph cluster_s2{
          style = "invis"
          S1 [shape=plaintext label = "Source 1"];
          S2 [shape=plaintext label = "Source 2"];
          S1->S2 [style="invis"]
      }
    subgraph cluster_p1 {
      label = "Worker 1";
        subgraph cluster_p1{
              label = "Partition 1"
              labelloc = "b"
          
              b1 [label = "B" color="red"]
              a1 [label = "A"]
              
          }
          
        subgraph cluster_pp1{
              label = "Partition 1"
          
               c[label = "C" color= "red"];
              a [label = "A"];
              
          }
          
    }
    subgraph cluster_p2 {
      label = "Worker 2";
      subgraph cluster_pp2{
              label = "Partition 2"
          b [label = "B" color="red"];
              d[label = "D"];
             
              
          }
          subgraph cluster_p2{
              label = "Partition 2"
              labelloc = "b"
              d1[label = "D"];
              c1[label = "C" color = "red"];
              
          }
          a->a1[style="dashed" dir="none"];
          b->b1[color="red" dir="none"];
          c->c1[color="red" dir="none"];
          d->d1[style="dashed" dir="none"];
    }
  }
} 
-----

== GlobalKTable

Реплицируется всюду целиком

[source,java]
----
GlobalKTable<...> global = streamsBuilder.globalTable("global", ...);
----

[graphviz, "globalktable.png"]
-----
digraph D {
  subgraph system {
     subgraph cluster_s2{
          style = "invis"
          S1 [shape=plaintext label = "Source 1"];
          S2 [shape=plaintext label = "GlobalKTable"];
          S1->S2 [style="invis"]
      }
    subgraph cluster_p1 {
      label = "Worker 1";
        subgraph cluster_p1{
              label = ""
          
              b1 [label = "B"]
              a1 [label = "A"]
              cc [label = "C"] 
              dd [label = "D"]
              a1->cc[style="invis"];
              b1->dd[style="invis"];
          }
          
        subgraph cluster_pp1{
              label = "Partition 1"
          
             
              a [label = "A"];
              b [label = "B"];
          }
          
    }
    subgraph cluster_p2 {
      label = "Worker 2";
      subgraph cluster_pp2{
              label = "Partition 2"
                c[label = "C"];
              
              d[label = "D"];
              
             
              
          }
          subgraph cluster_p2{
              label = ""
              labelloc = "b"
              d1[label = "D"];
              c1[label = "C" ];
              aa[label = "A"];
              bbb[label = "B"];
              c1->aa [style= "invis"];
              d1->bbb [style= "invis"];
              
          }
          a->a1[style="dashed" dir="none"];
          b->b1[style="dashed" dir="none"];
          c->c1[style="dashed" dir="none"];
          d->d1[style="dashed" dir="none"];
    }
  }
} 
-----

== Операции между стримами и таблицами: сводка
.Источник: https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#stateful-transformations[Kafka Streams DSL Documentation]
image::streams-stateful_operations.png[{image-70-width}]


== Сохранение Timestamped-значений в{nbsp}RocksDB

WindowKeySchema.java

[source,java]
----
static Bytes toStoreKeyBinary(final byte[] serializedKey,
                                  final long timestamp,
                                  final int seqnum) {
        final ByteBuffer buf = ByteBuffer.allocate(serializedKey.length
                                        + TIMESTAMP_SIZE 
                                        + SEQNUM_SIZE);
        buf.put(serializedKey);
        buf.putLong(timestamp);
        buf.putInt(seqnum);
        return Bytes.wrap(buf.array());
    }
----

== Быстрое извлечение

[graphviz, "timestamped-record.png"]
-----
digraph G
{
    node [shape=record, fontsize=18];
    node0 [label="..."];
    node1 [label="<f0> key|<f1> timestamp|<f2> seqnum"];
    node2 [label="..."];
    node0 -> node1;
    node0 -> node2;
}
-----


== Tumbling window

[source,java]
----
TimeWindowedKStream<String, String> windowed = 
    stream.groupByKey()
        .windowedBy(TimeWindows.of(Duration.ofSeconds(20)));
----
.Источник: Kafka Streams in Action
image::tumbling-window.png[{image-80-width}]

== Hopping Window
[source,java]
----
TimeWindowedKStream<String, String> windowed = 
    stream.groupByKey()
        .windowedBy(TimeWindows.of(Duration.ofSeconds(20))
                        .advanceBy(Duration.ofSeconds(10)));
----
.Источник: Kafka Streams in Action
image::hopping-window.png[{image-60-width}]

== Session Window
[source,java]
----
SessionWindowedKStream<String, String> windowed = 
    stream.groupByKey()
        .windowedBy(SessionWindows.with(Duration.ofMinutes(5)));
----
image::streams-session-windows-02.png[{image-60-width}]

== Оконные агрегации: демо
[source,java]
----
TimeWindowedKStream<String, String> windowed = filtered
    .groupByKey()
    .windowedBy(TimeWindows.of(Duration.ofSeconds(5)));
    
KTable<Windowed<String>, Long> count = windowed.count();
----
[source,code]
----
$kafka-topics --list
windows-demo-KSTREAM-AGGREGATE-STATE-STORE-0000000002-changelog
----
== Windowed<K> Interface

* `K key()`
* `Window window()`
** `Instant startTime()`
** `Instant endTime()`

== Windowed joins

Соединения стрим-стрим возможны в пределах временного окна:
[source,java]
----
KStream<String, String> join = 
  foo.join(bar, (v1, v2) -> String.format("%s: %s", v2, v1),
    JoinWindows.of(Duration.ofSeconds(20)));

----

(Анализ вида «увидел картинку и сделал покупку».)

== Kafka Streams in Action

[.custom-style]
[cols="30a,70a"]
|===
|image::KSIA.jpg[]
|
* **William Bejeck**, + 
“Kafka Streams in Action”, November 2018
* Примеры кода для Kafka 1.0
|===

== Kafka: The Definitive Guide

[.custom-style]
[cols="30a,70a"]
|===
|image::kafka-the-definitive-guide.jpg[]
|
* Gwen Shapira, Neha Narkhede, Todd Palino
* September 2017
|===



== Другие источники

- https://docs.confluent.io/current/streams/developer-guide/index.html[docs.confluent.io: Streams Developer Guide]
- https://www.confluent.io/blog/stream-processing-part-1-tutorial-developing-streaming-applications[Getting Your Feet Wet with Stream Processing (Confluent tutorials)]
- Исходники!
** https://github.com/apache/kafka/
** https://github.com/spring-projects/spring-kafka

== Телеграм

Грефневая Кафка

* https://t.me/AwesomeKafka_ru
* https://t.me/proKafka

== Выводы

[%step]
* Kafka StreamsAPI -- это большой мир
* Kafka StreamsAPI -- это удобная абстракция над «сырой» Кафкой
* Чтобы начать пользоваться, надо настроить мышление под потоковую обработку
* Технология переживает бурное развитие
** + живой community, есть шанс повлиять на процесс самому 
** - публичные интерфейсы изменяются очень быстро

== На этом всё!

* https://github.com/inponomarev/kstreams-examples
{nbsp} +
* Мой твиттер @inponomarev
{nbsp} +
* Мой email: ponomarev@corchestra.ru
{nbsp} +
* *Спасибо!*